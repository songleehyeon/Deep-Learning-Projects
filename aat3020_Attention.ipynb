{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRXWAXExlIAB",
        "outputId": "e7dbe0d5-45c4-4252-b4e6-52cbfa5151a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: koreanize-matplotlib in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from koreanize-matplotlib) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->koreanize-matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->koreanize-matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install transformers\n",
        "!pip install koreanize-matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import koreanize_matplotlib\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn.utils.rnn import PackedSequence, pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "import os\n",
        "\n",
        "# Below helps to run tokenizer with multiprocessing\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
      ],
      "metadata": {
        "id": "qVUVVQwWlQn1"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorization"
      ],
      "metadata": {
        "id": "dIzvNXHslYgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''example of vectorization of dot product for the two different sequence length'''\n",
        "e_states = torch.randn(100, 16)  # (input size, hidden size)\n",
        "d_states = torch.rand(80, 16)  # (output size, hidden size)\n",
        "\n",
        "dot_product = torch.mm(e_states, d_states.permute(1,0))  # (100, 16) x (16, 80) = (100, 80)\n",
        "dot_product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFVwIG4WlZqx",
        "outputId": "f12209bf-a75a-43d1-cb30-8833d74c7c2f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.2765, -0.0343, -4.8722,  ..., -3.9217, -4.8852, -2.4770],\n",
              "        [ 0.5759,  1.3688,  1.4649,  ...,  1.4977,  2.5791, -0.4470],\n",
              "        [ 4.3742,  3.6217,  2.2555,  ...,  5.5789,  3.7755,  2.9377],\n",
              "        ...,\n",
              "        [-0.8756, -2.6582, -3.0858,  ..., -1.9052, -4.6562, -3.1356],\n",
              "        [-0.0086, -2.1049, -1.0701,  ..., -0.7043,  0.0869, -1.4096],\n",
              "        [-4.2150, -3.2144, -5.7856,  ...,  1.0245, -3.6171, -3.2521]])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.) output sequence의 길이가 정해져있나?"
      ],
      "metadata": {
        "id": "d1DlHP7lmAyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Implement attention for a single batch\n"
      ],
      "metadata": {
        "id": "i2_RiZ15lZzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-1. Get attention score"
      ],
      "metadata": {
        "id": "zGNWoxaMvhn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_score_for_a_single_query(keys, query):\n",
        "      '''\n",
        "      returns an attention score for each vector in keys for a given query.\n",
        "      You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "      For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "      Arguments: keys (torch.Tensor): Has a shape of [T, C]\n",
        "                 query (torch.Tensor): Has a shape of [C]\n",
        "\n",
        "      Output: attention_score (torch.Tensor): Has a shape of [T]\n",
        "\n",
        "      attention_score[i] has to be a dot product value between keys[i] and query\n",
        "\n",
        "      TODO: Complete this sentence using torch.mm (matrix multiplication)\n",
        "      Hint: You can use atensor.unsqueeze(dim) to expand a dimension (with a diemsion of length 1) without changing item value of the tensor.\n",
        "      '''\n",
        "      att_score = torch.mm(keys, query.unsqueeze(1)).squeeze(1)\n",
        "      return att_score\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_t, h_size)\n",
        "query = torch.randn(h_size)\n",
        "\n",
        "att_score = get_attention_score_for_a_single_query(keys, query)\n",
        "att_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbCBBl5BlZ4b",
        "outputId": "a3ff0590-6dcb-444d-e5d1-5167c894f64a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-3.0786,  2.1729,  1.7950, -5.0503,  3.3254,  0.2828, -0.9800, -1.8868,\n",
              "         0.2550,  2.9389, -0.1799, -1.0586,  0.1465, -0.9441,  0.8888, -3.8108,\n",
              "        -2.5662, -1.1660, -2.2327,  2.7087, -0.5800,  8.7984,  4.3816])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-2. Get attention weight"
      ],
      "metadata": {
        "id": "dsOkeyWovmyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_weight_from_score(attention_score):\n",
        "    '''\n",
        "    converts attention score to attention weight.\n",
        "\n",
        "    Argument: attention_score (torch.Tensor): Has a shape of [T]\n",
        "\n",
        "    Output: attention_weight (torch.Tensor): Has a shape of [T]\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "    assert attention_score.ndim == 1\n",
        "    att_weight = torch.softmax(att_score, dim=0)\n",
        "\n",
        "    return att_weight\n",
        "\n",
        "att_weight = get_attention_weight_from_score(att_score)\n",
        "att_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZmhDI7JlaCV",
        "outputId": "d02cec22-825e-4015-a545-8c8ac1ebd792"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.7782e-06, 1.2936e-03, 8.8653e-04, 9.4370e-07, 4.0957e-03, 1.9541e-04,\n",
              "        5.5277e-05, 2.2321e-05, 1.9005e-04, 2.7829e-03, 1.2303e-04, 5.1099e-05,\n",
              "        1.7052e-04, 5.7296e-05, 3.5821e-04, 3.2593e-06, 1.1314e-05, 4.5893e-05,\n",
              "        1.5795e-05, 2.2107e-03, 8.2463e-05, 9.7556e-01, 1.1777e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-3. Get attention vector"
      ],
      "metadata": {
        "id": "vHsw8qcqvp5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_sum(values, attention_weight):\n",
        "    '''\n",
        "    converts attention score to attention weight\n",
        "\n",
        "    Argument: values (torch.Tensor): Has a shape of [T, C]\n",
        "              attention_weight: Has a shape of [T]\n",
        "\n",
        "    Output: attention_vector (torch.Tensor): Weighted sum of values using the attention weight. Has a shape of [C]\n",
        "\n",
        "    TODO: Complete this function using torch.mm\n",
        "    '''\n",
        "    att_vector = torch.mm(values.permute(1,0), attention_weight.unsqueeze(1)).squeeze(1)\n",
        "    return att_vector\n",
        "\n",
        "att_vec = get_weighted_sum(keys, att_weight) # In simple dot-product-attention, key and value are the same\n",
        "att_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUOR_eailYkK",
        "outputId": "18e4833c-9a0e-44d4-fe21-0fde2d1faedd"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.6280,  3.8540, -0.1042,  0.3148,  0.3711, -0.5095, -0.9663,  1.3295,\n",
              "         1.9003, -1.2611, -2.2939, -2.0338,  0.8757, -0.6726,  1.9071, -1.0711])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implement attention in Batch"
      ],
      "metadata": {
        "id": "yKHerjbhlQy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-1. Get attention score in batch"
      ],
      "metadata": {
        "id": "CwMV69_PwLMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_score_for_a_batch_query(keys, query):\n",
        "    '''\n",
        "    returns a batch of attention score for each vector in (multi-batch) keys for a given (single-batch) query.\n",
        "    You can regard 'keys' as hidden states over timestep of Encoder, while query is a hidden state of specific time step of Decoder\n",
        "    For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "    Arguments: keys (torch.Tensor): Has a shape of [N, T, C]\n",
        "               query (torch.Tensor): Has a shape of [N, C]\n",
        "\n",
        "    Output: attention_score (torch.Tensor): Has a shape of [N, T]\n",
        "            attention_score[n, i] has to be a dot product value between keys[n, i] and query[n]\n",
        "\n",
        "    TODO: Complete this function without using for loop\n",
        "    Hint: Use torch.bmm or torch.matmul after make two input tensors as 3-dim tensors.\n",
        "    '''\n",
        "    att_score = torch.bmm(keys, query.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "\n",
        "    return att_score\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_t = 23\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b,num_t, h_size)\n",
        "query = torch.randn(num_b, h_size)\n",
        "out = get_attention_score_for_a_batch_query(keys, query)"
      ],
      "metadata": {
        "id": "TbvETDRVvtc6"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2. Get attention score in batch\n",
        "- Implement the same function but in batchified queries"
      ],
      "metadata": {
        "id": "rN9spoahvtkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_score_for_a_batch_multiple_query(keys, queries):\n",
        "    '''\n",
        "    implement the attention score for not only single query, but multiple queries.\n",
        "\n",
        "    returns a batch of attention score for each vector in keys for given queries.\n",
        "    You can regard 'keys' as hidden states over timestep of Encoder, while querys are hidden states over timestep of Decoder\n",
        "    For every C-dimensional vector key, the attention score is a dot product between the key and the query vector.\n",
        "\n",
        "    Arguments: keys (torch.Tensor): Has a shape of [N, Ts, C].\n",
        "               queries (torch.Tensor): Has a shape of [N, Tt, C].\n",
        "\n",
        "    Output: attention_score (torch.Tensor): Has a shape of [N, Ts, Tt]\n",
        "            attention_score[n, i, t] has to be a dot product value between keys[n, i] and query[n, t]\n",
        "\n",
        "    TODO: Complete this function without using for loop\n",
        "    HINT: Use torch.bmm() with proper transpose (permutation) of given tensors. (You can use atensor.permute())\n",
        "          Think about which dimension (axis) of tensors has to be multiplied together and resolved (disappear) after matrix multiplication,\n",
        "          and how the result tensor has to look like (shape)\n",
        "    '''\n",
        "    att_score = torch.bmm(keys, queries.permute(0,2,1))\n",
        "    return att_score\n",
        "\n",
        "torch.manual_seed(0)\n",
        "num_b = 6\n",
        "num_ts = 23\n",
        "num_tt = 14\n",
        "h_size = 16\n",
        "\n",
        "keys = torch.randn(num_b, num_ts, h_size)\n",
        "queries = torch.randn(num_b, num_tt, h_size)\n",
        "att_score = get_attention_score_for_a_batch_multiple_query(keys, queries)\n",
        "\n",
        "att_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7XjvSlwvtoS",
        "outputId": "db5cf1ad-336d-4876-8437-2656bb41c672"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  6.1462,   2.7917,   3.2981,  ...,  -1.7558,  -1.9945,   1.7817],\n",
              "         [ -2.0761,   1.5621,  -4.6314,  ...,  -2.9616,   5.0151,  -0.5098],\n",
              "         [ -3.7923,   0.6755,  -2.5517,  ...,  -6.6489,   2.2012,   0.5882],\n",
              "         ...,\n",
              "         [  2.9819,  12.6860,   6.7435,  ...,   3.5522,  -7.0258,   2.3800],\n",
              "         [ -5.4682,  -2.9139,  -0.3054,  ...,   6.4960,  -1.4581, -12.5525],\n",
              "         [ -1.0037,   1.1092,   1.3248,  ...,   2.8827,   3.8804,  -5.4968]],\n",
              "\n",
              "        [[ -2.9907,  -0.1470,  -0.1703,  ...,   2.4992,  -1.8304,   1.1768],\n",
              "         [-15.4518,   2.2430,   4.9486,  ...,   4.4271,  -4.3865,  -9.2907],\n",
              "         [  4.2723,  -0.6171,   2.6252,  ...,  -2.2281,  -2.5648,  -4.1481],\n",
              "         ...,\n",
              "         [  1.3169,  -1.1141,  -1.6058,  ...,   1.9466,   2.6665,  -4.1625],\n",
              "         [ -7.9481,   5.0494,   0.7725,  ...,   0.5016,  -3.3123,  -7.8802],\n",
              "         [  1.8112,  -3.9315,   1.6521,  ...,  -0.2215,  -0.1541,  -6.4050]],\n",
              "\n",
              "        [[ -3.0427,   1.6135,  -0.4640,  ...,   5.9792,   2.2480,  -3.4328],\n",
              "         [ -5.7212,  -8.7972,  -5.4037,  ...,   3.7540,   0.4384,  -3.4441],\n",
              "         [  1.0249,   4.4344,   1.3304,  ...,  -1.3935,   2.5567,  -3.9180],\n",
              "         ...,\n",
              "         [ 10.2565,   6.2974,  -2.4342,  ..., -10.0146,   3.6096,   1.5109],\n",
              "         [  9.3230,  -1.0268,  11.0852,  ...,  -3.6774,   4.8100,  -2.3779],\n",
              "         [  6.1441,  -4.8747,   0.2777,  ...,   7.1300,  -3.4380,   1.1166]],\n",
              "\n",
              "        [[ -5.6300,   1.6126,   2.5171,  ...,   3.5575,   3.1118,   2.0304],\n",
              "         [  2.6453,  -0.9799,   0.6216,  ...,  -3.8061,   3.2136,  -0.5925],\n",
              "         [  3.2414,   5.6337,   3.7929,  ...,   0.3125,   1.4103,  -5.6959],\n",
              "         ...,\n",
              "         [ -1.5721,   2.2379,   4.6053,  ...,  -1.0397,  -3.7134,  -4.8251],\n",
              "         [ -4.5574,   3.2444,   2.8786,  ...,   1.6313,   0.1190,  -1.0030],\n",
              "         [ -1.4957,   0.2640,  -2.1453,  ...,  -2.5968,   0.1317,   2.6204]],\n",
              "\n",
              "        [[ -4.5165,  11.9102,  -1.3893,  ...,  -4.5206,   1.6530,   9.5429],\n",
              "         [  0.5686,  -5.3511,   2.3664,  ...,  -4.7446,   5.1878,   1.1045],\n",
              "         [  6.2639,   0.1570,  -0.1588,  ...,   2.1696,  -5.3117,   3.4494],\n",
              "         ...,\n",
              "         [ -3.1651,  -8.5573,  -2.6017,  ...,  -2.2827,   2.3201, -11.6823],\n",
              "         [  8.1099,   3.2290,  -7.5239,  ...,  -1.7162,  -8.0600,  -0.5222],\n",
              "         [  1.5312,  -0.7118,   5.2198,  ...,   3.8161,  -3.3262,  -2.4121]],\n",
              "\n",
              "        [[  4.3940,  -1.3437,  -0.7749,  ...,   2.1454,  -2.6441,   0.9273],\n",
              "         [ -4.2766,   2.7691,  -5.5275,  ...,  10.0679,  -3.9916,  10.4126],\n",
              "         [ -0.5561,  -5.9345,  -0.2756,  ...,  -2.7162,   2.0791,  -0.3644],\n",
              "         ...,\n",
              "         [  0.8519,   1.8976,   0.7429,  ...,   0.3895,   5.3467,   2.4843],\n",
              "         [ -1.2661,   4.2320,  -1.3352,  ...,   0.2109,  -4.6615,   2.1014],\n",
              "         [ -2.1632,   1.4186,  -0.8282,  ...,   2.8492,  -1.6369,  -0.5276]]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-3, Get masked softmax"
      ],
      "metadata": {
        "id": "sRE2bg2QlQ6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masked_softmax(attention_score, mask):\n",
        "    '''\n",
        "    During the batch computation, each sequence in the batch can have different length.\n",
        "    To group them as in a single tensor, we usually pad values\n",
        "\n",
        "    Arguments: attention_score (torch.Tensor): Has a shape of [N, Ts, Tt]\n",
        "               mask (torch.Tensor): Boolean tensor with a shape of [N, Ts] that represents whether the corresponding is valid or not.\n",
        "                                    mask[n, t] == 1 if and only if input_batch[n,t] is not a padded value.\n",
        "                                    If input_batch[n,t] is a padded value, then mask[n,t] == 0\n",
        "\n",
        "    Output: attention_weight (torch.Tensor): Has a shape of [N, Ts, Tt]\n",
        "                                             attention_weight[n, i, t] has to be an attention weight of values[n, i] for queries[n, t]\n",
        "\n",
        "    TODO: Complete this function without using for loop\n",
        "    Hint: You can give -infinity value by -float(\"inf\")\n",
        "\n",
        "    '''\n",
        "    assert attention_score.ndim == 3 and mask.ndim == 2\n",
        "\n",
        "    masked_att_score = attention_score.masked_fill(mask.unsqueeze(2)==0, -float('inf'))\n",
        "    att_weight = torch.softmax(masked_att_score, dim=1)\n",
        "    return att_weight\n",
        "\n",
        "\n",
        "'''\n",
        "Don't change this codes\n",
        "'''\n",
        "mask = torch.ones_like(att_score)[..., 0]\n",
        "mask[4, 15:] = 0\n",
        "mask[5, 17:] = 0\n",
        "att_score_modified = att_score.clone()\n",
        "att_score_modified[4, 15:] = 0\n",
        "attention_weight = get_masked_softmax(att_score, mask)\n",
        "attention_weight_for_modified = get_masked_softmax(att_score_modified, mask)\n",
        "attention_weight, attention_weight_for_modified"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv7UNVbqyN2G",
        "outputId": "88fa5ace-892e-402e-c475-aa12a12827a0"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[8.5095e-02, 5.0265e-05, 2.6409e-02,  ..., 1.8632e-04,\n",
              "           4.1884e-05, 1.4554e-03],\n",
              "          [2.2856e-05, 1.4698e-05, 9.5063e-06,  ..., 5.5794e-05,\n",
              "           4.6370e-02, 1.4716e-04],\n",
              "          [4.1080e-06, 6.0563e-06, 7.6066e-05,  ..., 1.3970e-06,\n",
              "           2.7809e-03, 4.4123e-04],\n",
              "          ...,\n",
              "          [3.5946e-03, 9.9608e-01, 8.2803e-01,  ..., 3.7625e-02,\n",
              "           2.7350e-07, 2.6474e-03],\n",
              "          [7.6878e-07, 1.6724e-07, 7.1906e-04,  ..., 7.1448e-01,\n",
              "           7.1609e-05, 8.6639e-10],\n",
              "          [6.6794e-05, 9.3445e-06, 3.6707e-03,  ..., 1.9264e-02,\n",
              "           1.4910e-02, 1.0046e-06]],\n",
              " \n",
              "         [[1.4556e-06, 7.2275e-05, 8.3620e-06,  ..., 7.2220e-02,\n",
              "           2.6008e-04, 2.2393e-03],\n",
              "          [5.6399e-12, 7.8883e-04, 1.3978e-03,  ..., 4.9651e-01,\n",
              "           2.0184e-05, 6.3696e-08],\n",
              "          [2.0765e-03, 4.5171e-05, 1.3690e-04,  ..., 6.3913e-04,\n",
              "           1.2479e-04, 1.0902e-05],\n",
              "          ...,\n",
              "          [1.0810e-04, 2.7478e-05, 1.9901e-06,  ..., 4.1556e-02,\n",
              "           2.3341e-02, 1.0746e-05],\n",
              "          [1.0234e-08, 1.3054e-02, 2.1468e-05,  ..., 9.7970e-03,\n",
              "           5.9093e-05, 2.6103e-07],\n",
              "          [1.7722e-04, 1.6422e-06, 5.1737e-05,  ..., 4.7540e-03,\n",
              "           1.3903e-03, 1.1412e-06]],\n",
              " \n",
              "         [[1.0078e-06, 4.8125e-03, 9.6156e-06,  ..., 1.0299e-02,\n",
              "           1.1371e-03, 1.3092e-04],\n",
              "          [6.9203e-08, 1.4490e-07, 6.8819e-08,  ..., 1.1127e-03,\n",
              "           1.8616e-04, 1.2944e-04],\n",
              "          [5.8871e-05, 8.0813e-02, 5.7844e-05,  ..., 6.4695e-06,\n",
              "           1.5483e-03, 8.0587e-05],\n",
              "          ...,\n",
              "          [6.0139e-01, 5.2067e-01, 1.3407e-06,  ..., 1.1662e-09,\n",
              "           4.4373e-03, 1.8367e-02],\n",
              "          [2.3644e-01, 3.4332e-04, 9.9705e-01,  ..., 6.5917e-07,\n",
              "           1.4739e-02, 3.7597e-04],\n",
              "          [9.8438e-03, 7.3208e-06, 2.0188e-05,  ..., 3.2553e-02,\n",
              "           3.8583e-06, 1.2382e-02]],\n",
              " \n",
              "         [[3.3037e-07, 1.4234e-03, 2.2441e-03,  ..., 1.1613e-01,\n",
              "           1.2482e-01, 4.2618e-04],\n",
              "          [1.2970e-03, 1.0651e-04, 3.3717e-04,  ..., 7.3622e-05,\n",
              "           1.3819e-01, 3.0935e-05],\n",
              "          [2.3540e-03, 7.9368e-02, 8.0376e-03,  ..., 4.5255e-03,\n",
              "           2.2768e-02, 1.8797e-07],\n",
              "          ...,\n",
              "          [1.9115e-05, 2.6601e-03, 1.8111e-02,  ..., 1.1706e-03,\n",
              "           1.3556e-04, 4.4903e-07],\n",
              "          [9.6572e-07, 7.2776e-03, 3.2214e-03,  ..., 1.6921e-02,\n",
              "           6.2594e-03, 2.0521e-05],\n",
              "          [2.0632e-05, 3.6950e-04, 2.1194e-05,  ..., 2.4671e-04,\n",
              "           6.3394e-03, 7.6880e-04]],\n",
              " \n",
              "         [[5.4761e-06, 9.9943e-01, 7.3903e-05,  ..., 3.6373e-06,\n",
              "           2.0599e-02, 9.4030e-01],\n",
              "          [8.8486e-04, 3.1861e-08, 3.1606e-03,  ..., 2.9076e-06,\n",
              "           7.0629e-01, 2.0348e-04],\n",
              "          [2.6322e-01, 7.8598e-06, 2.5298e-04,  ..., 2.9262e-03,\n",
              "           1.9459e-05, 2.1226e-03],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              " \n",
              "         [[2.5799e-01, 7.1575e-05, 5.9149e-03,  ..., 3.4783e-04,\n",
              "           3.0966e-04, 7.3722e-05],\n",
              "          [4.4262e-05, 4.3747e-03, 5.1039e-05,  ..., 9.5951e-01,\n",
              "           8.0478e-05, 9.7059e-01],\n",
              "          [1.8274e-03, 7.2610e-07, 9.7449e-03,  ..., 2.6915e-06,\n",
              "           3.4847e-02, 2.0260e-05],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]]]),\n",
              " tensor([[[8.5095e-02, 5.0265e-05, 2.6409e-02,  ..., 1.8632e-04,\n",
              "           4.1884e-05, 1.4554e-03],\n",
              "          [2.2856e-05, 1.4698e-05, 9.5063e-06,  ..., 5.5794e-05,\n",
              "           4.6370e-02, 1.4716e-04],\n",
              "          [4.1080e-06, 6.0563e-06, 7.6066e-05,  ..., 1.3970e-06,\n",
              "           2.7809e-03, 4.4123e-04],\n",
              "          ...,\n",
              "          [3.5946e-03, 9.9608e-01, 8.2803e-01,  ..., 3.7625e-02,\n",
              "           2.7350e-07, 2.6474e-03],\n",
              "          [7.6878e-07, 1.6724e-07, 7.1906e-04,  ..., 7.1448e-01,\n",
              "           7.1609e-05, 8.6639e-10],\n",
              "          [6.6794e-05, 9.3445e-06, 3.6707e-03,  ..., 1.9264e-02,\n",
              "           1.4910e-02, 1.0046e-06]],\n",
              " \n",
              "         [[1.4556e-06, 7.2275e-05, 8.3620e-06,  ..., 7.2220e-02,\n",
              "           2.6008e-04, 2.2393e-03],\n",
              "          [5.6399e-12, 7.8883e-04, 1.3978e-03,  ..., 4.9651e-01,\n",
              "           2.0184e-05, 6.3696e-08],\n",
              "          [2.0765e-03, 4.5171e-05, 1.3690e-04,  ..., 6.3913e-04,\n",
              "           1.2479e-04, 1.0902e-05],\n",
              "          ...,\n",
              "          [1.0810e-04, 2.7478e-05, 1.9901e-06,  ..., 4.1556e-02,\n",
              "           2.3341e-02, 1.0746e-05],\n",
              "          [1.0234e-08, 1.3054e-02, 2.1468e-05,  ..., 9.7970e-03,\n",
              "           5.9093e-05, 2.6103e-07],\n",
              "          [1.7722e-04, 1.6422e-06, 5.1737e-05,  ..., 4.7540e-03,\n",
              "           1.3903e-03, 1.1412e-06]],\n",
              " \n",
              "         [[1.0078e-06, 4.8125e-03, 9.6156e-06,  ..., 1.0299e-02,\n",
              "           1.1371e-03, 1.3092e-04],\n",
              "          [6.9203e-08, 1.4490e-07, 6.8819e-08,  ..., 1.1127e-03,\n",
              "           1.8616e-04, 1.2944e-04],\n",
              "          [5.8871e-05, 8.0813e-02, 5.7844e-05,  ..., 6.4695e-06,\n",
              "           1.5483e-03, 8.0587e-05],\n",
              "          ...,\n",
              "          [6.0139e-01, 5.2067e-01, 1.3407e-06,  ..., 1.1662e-09,\n",
              "           4.4373e-03, 1.8367e-02],\n",
              "          [2.3644e-01, 3.4332e-04, 9.9705e-01,  ..., 6.5917e-07,\n",
              "           1.4739e-02, 3.7597e-04],\n",
              "          [9.8438e-03, 7.3208e-06, 2.0188e-05,  ..., 3.2553e-02,\n",
              "           3.8583e-06, 1.2382e-02]],\n",
              " \n",
              "         [[3.3037e-07, 1.4234e-03, 2.2441e-03,  ..., 1.1613e-01,\n",
              "           1.2482e-01, 4.2618e-04],\n",
              "          [1.2970e-03, 1.0651e-04, 3.3717e-04,  ..., 7.3622e-05,\n",
              "           1.3819e-01, 3.0935e-05],\n",
              "          [2.3540e-03, 7.9368e-02, 8.0376e-03,  ..., 4.5255e-03,\n",
              "           2.2768e-02, 1.8797e-07],\n",
              "          ...,\n",
              "          [1.9115e-05, 2.6601e-03, 1.8111e-02,  ..., 1.1706e-03,\n",
              "           1.3556e-04, 4.4903e-07],\n",
              "          [9.6572e-07, 7.2776e-03, 3.2214e-03,  ..., 1.6921e-02,\n",
              "           6.2594e-03, 2.0521e-05],\n",
              "          [2.0632e-05, 3.6950e-04, 2.1194e-05,  ..., 2.4671e-04,\n",
              "           6.3394e-03, 7.6880e-04]],\n",
              " \n",
              "         [[5.4761e-06, 9.9943e-01, 7.3903e-05,  ..., 3.6373e-06,\n",
              "           2.0599e-02, 9.4030e-01],\n",
              "          [8.8486e-04, 3.1861e-08, 3.1606e-03,  ..., 2.9076e-06,\n",
              "           7.0629e-01, 2.0348e-04],\n",
              "          [2.6322e-01, 7.8598e-06, 2.5298e-04,  ..., 2.9262e-03,\n",
              "           1.9459e-05, 2.1226e-03],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              " \n",
              "         [[2.5799e-01, 7.1575e-05, 5.9149e-03,  ..., 3.4783e-04,\n",
              "           3.0966e-04, 7.3722e-05],\n",
              "          [4.4262e-05, 4.3747e-03, 5.1039e-05,  ..., 9.5951e-01,\n",
              "           8.0478e-05, 9.7059e-01],\n",
              "          [1.8274e-03, 7.2610e-07, 9.7449e-03,  ..., 2.6915e-06,\n",
              "           3.4847e-02, 2.0260e-05],\n",
              "          ...,\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-4. Implement weighted sum in batchified version"
      ],
      "metadata": {
        "id": "Cp-5KR2TyN9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_weighted_sum(values, attention_weight):\n",
        "    '''\n",
        "    Argument: values (torch.Tensor): Has a shape of [N, Ts, C]\n",
        "              attention_weight: Has a shape of [N, Ts, Tt],\n",
        "                                attention_weight[n, s, t] represents weight for value[n, s] that corresponds to a given query, queries[n, t]\n",
        "\n",
        "    Output: attention_vector (torch.Tensor): Has a shape of [N, Tt, C]\n",
        "\n",
        "    TODO: Complete this function using torch.bmm\n",
        "    '''\n",
        "    att_vector = torch.bmm(attention_weight.permute(0,2,1), values)\n",
        "    return att_vector\n",
        "\n",
        "att_out = get_batch_weighted_sum(keys, attention_weight)\n",
        "att_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiK5-ulayOBa",
        "outputId": "216ac598-2fde-494f-f7cd-1f1780396447"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.3373e-01, -2.0938e+00, -4.5334e-02,  ...,  9.5458e-01,\n",
              "           1.0229e+00,  1.1447e+00],\n",
              "         [-1.3264e-01, -5.9684e-02,  4.0668e-01,  ..., -2.2277e+00,\n",
              "           1.4654e+00, -1.2140e+00],\n",
              "         [-2.1483e-01, -3.9624e-02,  3.4638e-01,  ..., -1.8784e+00,\n",
              "           1.3381e+00, -1.0378e+00],\n",
              "         ...,\n",
              "         [ 4.7269e-01,  2.7082e+00, -1.5966e-01,  ..., -8.5101e-01,\n",
              "           1.6295e+00, -6.9686e-01],\n",
              "         [ 1.3049e+00, -7.9595e-01,  5.8837e-01,  ..., -4.2750e-01,\n",
              "           9.4185e-02, -6.0422e-01],\n",
              "         [-2.1447e-01,  4.5967e-01,  9.0154e-01,  ..., -1.1010e+00,\n",
              "           1.5383e+00, -6.2885e-01]],\n",
              "\n",
              "        [[-2.0093e-02,  9.5785e-01, -1.0546e+00,  ..., -5.6727e-01,\n",
              "           9.5331e-01, -1.4754e+00],\n",
              "         [ 2.6579e-01, -9.2264e-01,  8.4929e-01,  ...,  2.4586e+00,\n",
              "          -2.5894e+00,  2.3314e+00],\n",
              "         [ 5.3756e-01, -9.0356e-01,  1.7743e-01,  ...,  2.2658e+00,\n",
              "           3.0777e-01,  2.0951e+00],\n",
              "         ...,\n",
              "         [ 1.3244e-01,  8.5970e-01,  2.4145e-01,  ..., -1.7893e-01,\n",
              "           9.2332e-01, -6.8233e-01],\n",
              "         [-8.8633e-02,  5.1850e-01, -1.4503e+00,  ..., -5.1185e-01,\n",
              "           5.6031e-01,  8.2076e-01],\n",
              "         [-1.1267e-01, -5.4648e-01, -4.8170e-02,  ...,  1.8755e+00,\n",
              "          -3.0700e-01,  9.4322e-01]],\n",
              "\n",
              "        [[-6.1817e-01,  2.6989e-01, -1.7159e+00,  ..., -1.2042e+00,\n",
              "          -1.3739e+00,  6.5000e-01],\n",
              "         [ 8.7003e-01,  8.7786e-02, -1.3764e+00,  ..., -1.0438e+00,\n",
              "          -1.3338e-01,  4.9795e-01],\n",
              "         [-5.9019e-01,  4.2505e-01, -1.2334e+00,  ..., -7.7825e-01,\n",
              "          -2.1349e+00,  3.3232e-01],\n",
              "         ...,\n",
              "         [-4.5776e-01,  5.3027e-01,  8.0297e-01,  ...,  1.1601e+00,\n",
              "          -4.1216e-01, -8.4225e-01],\n",
              "         [-2.7609e-01,  5.8881e-01,  5.0533e-01,  ..., -5.1114e-01,\n",
              "           1.7841e+00, -5.5431e-01],\n",
              "         [ 2.9698e-02, -1.3222e+00, -7.9683e-01,  ...,  3.8490e-01,\n",
              "          -1.0175e+00,  1.4906e-01]],\n",
              "\n",
              "        [[ 8.3192e-01,  1.6496e-01, -9.2035e-01,  ..., -1.6308e+00,\n",
              "          -5.2375e-01, -1.1106e+00],\n",
              "         [-7.9787e-01, -3.0595e-01, -9.7095e-01,  ...,  2.6117e-03,\n",
              "           2.1266e-01,  7.9024e-01],\n",
              "         [ 3.6286e-01,  6.3939e-01,  3.0618e-01,  ...,  1.3938e+00,\n",
              "           2.0934e+00,  1.1223e+00],\n",
              "         ...,\n",
              "         [ 4.2950e-01,  2.7868e-01, -6.7143e-01,  ..., -4.3766e-01,\n",
              "          -7.7644e-01, -2.6162e-01],\n",
              "         [-8.4428e-01, -5.4181e-02, -1.7641e-01,  ...,  1.1040e-01,\n",
              "           1.6086e-01,  7.0424e-01],\n",
              "         [ 1.2658e+00,  1.9420e+00, -1.2539e-01,  ...,  1.0030e+00,\n",
              "          -1.3475e-01, -5.0187e-01]],\n",
              "\n",
              "        [[ 5.8191e-03, -5.3975e-01, -3.6032e-01,  ...,  5.1161e-01,\n",
              "           1.4710e-01,  1.0211e+00],\n",
              "         [ 3.7232e+00,  1.7317e+00, -7.4160e-01,  ..., -4.5058e-01,\n",
              "          -2.1989e+00,  1.4684e+00],\n",
              "         [-7.7349e-01,  1.1662e+00, -1.5678e+00,  ..., -5.2115e-01,\n",
              "           1.4653e+00, -1.6642e+00],\n",
              "         ...,\n",
              "         [-4.5703e-01,  1.6784e+00, -6.8822e-02,  ..., -3.6136e-01,\n",
              "          -6.9498e-01, -6.5073e-01],\n",
              "         [-1.2242e+00,  8.1537e-01,  1.2802e+00,  ...,  2.8726e-01,\n",
              "          -6.7655e-01,  9.0936e-01],\n",
              "         [ 3.4806e+00,  1.7345e+00, -6.9836e-01,  ..., -4.4237e-01,\n",
              "          -2.1103e+00,  1.3477e+00]],\n",
              "\n",
              "        [[ 7.4298e-01,  6.8085e-01, -4.0109e-01,  ..., -6.9539e-01,\n",
              "          -7.1683e-01,  1.3663e-01],\n",
              "         [-1.7923e-01, -4.7257e-01, -4.0078e-01,  ..., -8.6471e-01,\n",
              "           1.2872e-01, -2.4034e-02],\n",
              "         [-3.2743e-01, -1.8710e-01,  6.0103e-02,  ...,  6.2887e-01,\n",
              "          -8.4385e-02,  4.1515e-01],\n",
              "         ...,\n",
              "         [-2.2279e-01,  2.0214e-01, -1.1098e+00,  ..., -2.4794e+00,\n",
              "          -1.3261e+00,  1.8497e+00],\n",
              "         [-4.9566e-01, -1.3428e+00,  4.1454e-01,  ..., -7.3784e-01,\n",
              "           3.7670e-01, -1.1959e-01],\n",
              "         [-2.2121e-01,  1.5855e-01, -1.1392e+00,  ..., -2.5089e+00,\n",
              "          -1.3221e+00,  1.8876e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Make seq2seq with attention\n",
        "- Using Pre-defined `TranslatorBi` class, complete a new `TranslatorAtt` class"
      ],
      "metadata": {
        "id": "fLUMb8P0ylal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-0. Prepare dataset and tokenizer"
      ],
      "metadata": {
        "id": "TZCpJVN1yleW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset (originally from NIA AI-Hub)\n",
        "!gdown 13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
        "!unzip nia_korean_english_csv.zip --q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW3FkWxN07lp",
        "outputId": "564f4634-880c-42cb-e99d-32953a16814f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13CGLEULYccogSLByHXPAxSveLZTtnj8c\n",
            "From (redirected): https://drive.google.com/uc?id=13CGLEULYccogSLByHXPAxSveLZTtnj8c&confirm=t&uuid=6662b6d2-dbea-472e-980b-478f8fd091f6\n",
            "To: /content/nia_korean_english_csv.zip\n",
            "100% 190M/190M [00:02<00:00, 68.2MB/s]\n",
            "Archive:  nia_korean_english_csv.zip\n",
            "caution: filename not matched:  --q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip --q 'nia_korean_english_csv.zip' -d '/content'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BglPHJ-2gLi",
        "outputId": "529b4729-fbaa-4542-877c-4eeffb4d48e2"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nia_korean_english_csv.zip\n",
            "replace /content/nia_korean_english.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/nia_korean_english.csv  y\n",
            "\n",
            "replace /content/hugging_eng_32000/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: /content/hugging_eng_32000/vocab.txt  \n",
            "replace /content/hugging_kor_32000/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data and tokenizer\n",
        "df = pd.read_csv('nia_korean_english.csv')\n",
        "\n",
        "src_tokenizer = BertTokenizerFast.from_pretrained('hugging_kor_32000',\n",
        "                                                  strip_accents=False,\n",
        "                                                  lower_case=False)\n",
        "tgt_tokenizer = BertTokenizerFast.from_pretrained('hugging_eng_32000',\n",
        "                                                  strip_accents=False,\n",
        "                                                  lower_case=False)"
      ],
      "metadata": {
        "id": "mOcUeGQo07pC"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "fmOCrGad4iWR",
        "outputId": "8b7e4f51-d689-4d46-cf37-d0b5eda8ee1e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  원문  \\\n",
              "0  'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
              "1                                       씨티은행에서 일하세요?   \n",
              "2              푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
              "3   11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
              "4     6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
              "\n",
              "                                                 번역문  \n",
              "0  Bible Coloring' is a coloring application that...  \n",
              "1                        Do you work at a City bank?  \n",
              "2  PURITO's bestseller, which recorded 4th rough ...  \n",
              "3  In Chapter 11 Jesus called Lazarus from the to...  \n",
              "4  I would feel grateful to know how many stocks ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-866675b1-8d95-41f5-b82a-ba4eaedcf6d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>원문</th>\n",
              "      <th>번역문</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...</td>\n",
              "      <td>Bible Coloring' is a coloring application that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>씨티은행에서 일하세요?</td>\n",
              "      <td>Do you work at a City bank?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.</td>\n",
              "      <td>PURITO's bestseller, which recorded 4th rough ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.</td>\n",
              "      <td>In Chapter 11 Jesus called Lazarus from the to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.</td>\n",
              "      <td>I would feel grateful to know how many stocks ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-866675b1-8d95-41f5-b82a-ba4eaedcf6d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-866675b1-8d95-41f5-b82a-ba4eaedcf6d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-866675b1-8d95-41f5-b82a-ba4eaedcf6d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ee77f085-f1c4-4669-b8a9-b0305a44aa41\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee77f085-f1c4-4669-b8a9-b0305a44aa41')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ee77f085-f1c4-4669-b8a9-b0305a44aa41 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationSet:\n",
        "    def __init__(self, df, src_tokenizer, tgt_tokenizer):\n",
        "        self.data = df\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        selected_row = self.data.iloc[idx]\n",
        "        source = selected_row['원문']\n",
        "        target = selected_row['번역문']\n",
        "\n",
        "        source_enc = self.src_tokenizer(source)['input_ids']\n",
        "        target_enc = self.tgt_tokenizer(target)['input_ids']\n",
        "\n",
        "        return torch.LongTensor(source_enc), torch.LongTensor(target_enc[:-1]), torch.LongTensor(target_enc[1:])\n",
        "\n",
        "\n",
        "entire_set = TranslationSet(df, src_tokenizer, tgt_tokenizer)\n",
        "train_set, valid_set, test_set = torch.utils.data.random_split(entire_set, [int(len(entire_set)*0.9), int(len(entire_set)*0.05), len(entire_set)-int(len(entire_set)*0.9)-int(len(entire_set)*0.05)])\n",
        "\n",
        "print(f\"Data Item Example: {entire_set[0]}\\n\")\n",
        "print(f\"Length of split: {len(train_set)}, {len(valid_set)}, {len(test_set)} (train, valid, test)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5H3p3ef07vv",
        "outputId": "cbce9ef3-0c95-4111-b241-10d2431121b8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Item Example: (tensor([    2,    11,    70,  4665,  5209, 13306,    71, 12901,  9565, 12435,\n",
            "           11,  3546, 14567,  4325,  8934,  8407,  7400,  4154,  3252,  6420,\n",
            "        12985,  4996,  3397,  6461,    18,     3]), tensor([    2, 26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,\n",
            "         1117,  1042,  2405,  4024,  5520,  1039,  1023, 26268,    18]), tensor([26268, 23067,    11,  1056,    69, 23067,  2803,  1067,  5155,  1117,\n",
            "         1042,  2405,  4024,  5520,  1039,  1023, 26268,    18,     3]))\n",
            "\n",
            "Length of split: 1442176, 80120, 80122 (train, valid, test)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_collate(raw_batch):\n",
        "    source, target, shifted_target = zip(*raw_batch)\n",
        "    return pack_sequence(source, enforce_sorted=False), pack_sequence(target, enforce_sorted=False), pack_sequence(shifted_target, enforce_sorted=False)\n",
        "\n",
        "single_loader = DataLoader(train_set, batch_size=1, collate_fn=pack_collate, shuffle=True, num_workers=4)\n",
        "train_loader = DataLoader(train_set, batch_size=64, collate_fn=pack_collate, shuffle=True, num_workers=4)\n",
        "valid_loader = DataLoader(valid_set, batch_size=128, collate_fn=pack_collate, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=128, collate_fn=pack_collate, shuffle=True, num_workers=4)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuOb12iUylmb",
        "outputId": "6d05b241-e109-403d-d7bb-0472c9f8c64b"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PackedSequence(data=tensor([2, 2, 2,  ..., 3, 3, 3]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 60, 57, 57, 56, 54, 51, 50, 49, 47,\n",
              "         40, 37, 34, 34, 34, 33, 32, 30, 27, 25, 21, 20, 16, 15, 13, 13, 12, 11,\n",
              "         11, 10,  8,  7,  6,  5,  5,  4,  4,  4,  2]), sorted_indices=tensor([19, 10, 63, 40, 56, 47,  2, 50, 42, 21, 61, 54, 20,  8, 29, 45,  6, 12,\n",
              "         17, 37, 53, 39, 18, 22, 15,  3, 34, 27,  4, 35, 46, 30, 51, 60, 32, 52,\n",
              "         33, 23, 14, 38, 16,  1, 59, 58, 55, 44, 43, 26,  0, 36,  5, 28,  9, 11,\n",
              "         25, 13, 48, 57, 62, 31, 49, 41,  7, 24]), unsorted_indices=tensor([48, 41,  6, 25, 28, 50, 16, 62, 13, 52,  1, 53, 17, 55, 38, 24, 40, 18,\n",
              "         22,  0, 12,  9, 23, 37, 63, 54, 47, 27, 51, 14, 31, 59, 34, 36, 26, 29,\n",
              "         49, 19, 39, 21,  3, 61,  8, 46, 45, 15, 30,  5, 56, 60,  7, 32, 35, 20,\n",
              "         11, 44,  4, 57, 43, 42, 33, 10, 58,  2])),\n",
              " PackedSequence(data=tensor([  2,   2,   2,  ...,  18, 269,   6]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 62, 62, 62, 60, 59, 58, 58, 54, 51, 47, 45,\n",
              "         41, 39, 39, 36, 35, 35, 34, 33, 33, 33, 32, 30, 29, 27, 26, 24, 23, 21,\n",
              "         20, 18, 18, 17, 16, 15, 13, 12, 12, 12, 12, 11, 11, 10, 10,  8,  8,  7,\n",
              "          5,  4,  4,  4,  3,  3,  3,  2,  1]), sorted_indices=tensor([10, 54, 37, 61, 21, 19,  6, 12, 63, 18,  8, 47, 45, 56, 42, 40, 50, 29,\n",
              "         30, 34, 46, 51, 35, 39, 38, 60, 20, 15, 27, 53, 17,  2, 32,  3,  4,  5,\n",
              "         33,  1, 44, 59, 58,  9, 14, 16, 22, 52, 36, 43, 23, 55, 31, 57,  0, 26,\n",
              "         11, 48, 25, 28, 13,  7, 41, 62, 49, 24]), unsorted_indices=tensor([52, 37, 31, 33, 34, 35,  6, 59, 10, 41,  0, 54,  7, 58, 42, 27, 43, 30,\n",
              "          9,  5, 26,  4, 44, 48, 63, 56, 53, 28, 57, 17, 18, 50, 32, 36, 19, 22,\n",
              "         46,  2, 24, 23, 15, 60, 14, 47, 38, 12, 20, 11, 55, 62, 16, 21, 45, 29,\n",
              "          1, 49, 13, 51, 40, 39, 25,  3, 61,  8])),\n",
              " PackedSequence(data=tensor([27608, 25719,  1322,  ...,     6,     3,     3]), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 62, 62, 62, 60, 59, 58, 58, 54, 51, 47, 45,\n",
              "         41, 39, 39, 36, 35, 35, 34, 33, 33, 33, 32, 30, 29, 27, 26, 24, 23, 21,\n",
              "         20, 18, 18, 17, 16, 15, 13, 12, 12, 12, 12, 11, 11, 10, 10,  8,  8,  7,\n",
              "          5,  4,  4,  4,  3,  3,  3,  2,  1]), sorted_indices=tensor([10, 54, 37, 61, 21, 19,  6, 12, 63, 18,  8, 47, 45, 56, 42, 40, 50, 29,\n",
              "         30, 34, 46, 51, 35, 39, 38, 60, 20, 15, 27, 53, 17,  2, 32,  3,  4,  5,\n",
              "         33,  1, 44, 59, 58,  9, 14, 16, 22, 52, 36, 43, 23, 55, 31, 57,  0, 26,\n",
              "         11, 48, 25, 28, 13,  7, 41, 62, 49, 24]), unsorted_indices=tensor([52, 37, 31, 33, 34, 35,  6, 59, 10, 41,  0, 54,  7, 58, 42, 27, 43, 30,\n",
              "          9,  5, 26,  4, 44, 48, 63, 56, 53, 28, 57, 17, 18, 50, 32, 36, 19, 22,\n",
              "         46,  2, 24, 23, 15, 60, 14, 47, 38, 12, 20, 11, 55, 62, 16, 21, 45, 29,\n",
              "          1, 49, 13, 51, 40, 39, 25,  3, 61,  8])))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, loss_fn, train_loader, valid_loader, device):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.best_valid_accuracy = 0\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "        self.validation_acc = []\n",
        "\n",
        "\n",
        "    def save_model(self, path='kor_eng_translator_attention_model.pt'):\n",
        "        torch.save({'model': self.model.state_dict(), 'optim': self.optimizer.state_dict()}, path)\n",
        "\n",
        "\n",
        "    def train_by_num_epoch(self, num_epochs):\n",
        "        for epoch in tqdm(range(num_epochs)):\n",
        "            self.model.train()\n",
        "            with tqdm(self.train_loader, leave=False) as pbar:\n",
        "                for batch in pbar:\n",
        "                    loss = self.train_by_single_batch(batch)\n",
        "                    self.training_loss.append(loss)\n",
        "                    pbar.set_description(f\"Epoch {epoch+1}, Loss {loss:.4f}\")\n",
        "\n",
        "            self.model.eval()\n",
        "            validation_loss,validation_acc = self.validate()\n",
        "            self.validation_loss.append(validation_loss)\n",
        "            self.validation_acc.append(validation_acc)\n",
        "\n",
        "            self.best_valid_accuracy = max(validation_acc, self.best_valid_accuracy)\n",
        "            self.save_model('kor_eng_translator_attention_model_last.pt')\n",
        "\n",
        "\n",
        "\n",
        "    def train_by_single_batch(self, batch):\n",
        "        '''\n",
        "        batch (tuple): (batch_of_input_text, batch_of_label)\n",
        "\n",
        "        output: loss (float): Mean binary cross entropy value for every sample in the training batch\n",
        "        '''\n",
        "        src, tgt, shifted_tft = batch\n",
        "        src = src.to(self.device)\n",
        "        tgt = tgt.to(self.device)\n",
        "        shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "        output = self.model(src, tgt)\n",
        "\n",
        "        if isinstance(output, PackedSequence):\n",
        "            loss = self.loss_fn(output.data, shifted_tgt.data)\n",
        "        else:\n",
        "            loss = self.loss_fn(output, shifted_tgt)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def validate(self, external_loader=None):\n",
        "        '''\n",
        "        input: data_loader\n",
        "\n",
        "        output: validation_loss (float): Mean Binary Cross Entropy value for every sample in validation set\n",
        "                validation_accuracy (float): Mean Accuracy value for every sample in validation set\n",
        "\n",
        "        '''\n",
        "        if external_loader and isinstance(external_loader, DataLoader):\n",
        "            loader = external_loader\n",
        "            print('An arbitrary loader is used instead of Validation loader')\n",
        "        else:\n",
        "            loader = self.valid_loader\n",
        "\n",
        "        self.model_eval()\n",
        "\n",
        "        validation_loss = 0\n",
        "        num_correct = 0\n",
        "        num_data = 0\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for batch in loader:\n",
        "                src, tgt, shifted_tgt = batch\n",
        "                src = src.to(self.device)\n",
        "                tgt = tgt.to(self.device)\n",
        "                shifted_tgt = shifted_tgt.to(self.device)\n",
        "\n",
        "                output = self.model(src, tgt)\n",
        "\n",
        "                if isinstance(output, PackedSequence):\n",
        "                    loss = self.loss_fn(output.data, shifted_tgt.data)\n",
        "                else:\n",
        "                    loss = self.loss_fn(output, shifted_tgt)\n",
        "\n",
        "                validation_loss += loss\n",
        "\n",
        "                if isinstance(output, PackedSequence):\n",
        "                    num_correct += (output.data.argmax(dim=-1) == shifted_tgt.data).sum().item()\n",
        "                else:\n",
        "                    num_correct += (output.argmax(dim=-1) == shifted_tgt).sum().item()\n",
        "\n",
        "                num_data += len(output.data)\n",
        "\n",
        "        return validation_loss / num_data, num_correct / num_data\n",
        "\n",
        "\n",
        "\n",
        "    def nll_loss(self, output, target, eps=1e-8):\n",
        "        '''\n",
        "        for PackedSequence, the input is 2D tensor\n",
        "\n",
        "        predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
        "        indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
        "        '''\n",
        "\n"
      ],
      "metadata": {
        "id": "VstuVq6pbpZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nll_loss(pred, target, eps=1e-8):\n",
        "  '''\n",
        "  for PackedSequence, the input is 2D tensor\n",
        "\n",
        "  predicted_prob_distribution has a shape of [num_entire_tokens_in_the_batch x vocab_size]\n",
        "  indices_of_correct_token has a shape of [num_entire_tokens_in_the_batch]\n",
        "  '''\n",
        "\n",
        "  if pred.ndim == 3:\n",
        "    pred = pred.flatten(0, 1)\n",
        "  if target.ndim == 2:\n",
        "    target = target.flatten(0, 1)\n",
        "  assert pred.ndim == 2\n",
        "  assert target.ndim == 1\n",
        "  return -torch.log(pred[torch.arange(len(target)), target] + eps).mean()\n"
      ],
      "metadata": {
        "id": "eA-vIA_66UQH"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Pre-defined class\n",
        "\n",
        "You don't need to change this code\n",
        "'''\n",
        "class TranslatorBi(nn.Module):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=256, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.src_tokenizer = src_tokenizer\n",
        "    self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    self.src_vocab_size = self.src_tokenizer.vocab_size\n",
        "    self.tgt_vocab_size = self.tgt_tokenizer.vocab_size\n",
        "\n",
        "    self.src_embedder = nn.Embedding(self.src_vocab_size, hidden_size)\n",
        "    self.tgt_embedder = nn.Embedding(self.tgt_vocab_size, hidden_size)\n",
        "\n",
        "    self.encoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "    self.decoder = nn.GRU(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "    self.decoder_proj = nn.Linear(hidden_size, self.tgt_vocab_size)\n",
        "\n",
        "  def run_encoder(self, x):\n",
        "    if isinstance(x, PackedSequence):\n",
        "      emb_x = PackedSequence(self.src_embedder(x.data), batch_sizes=x.batch_sizes, sorted_indices=x.sorted_indices, unsorted_indices=x.unsorted_indices)\n",
        "    else:\n",
        "      emb_x = self.src_embedder(x)\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden = self.encoder(emb_x)\n",
        "\n",
        "    # Because we use bi-directional GRU, there are (num_layers * 2) last hidden states\n",
        "    # Here, we make it to (num_layers) last hidden states by taking mean of [left-to-right-GRU] and [right-to-left-GRU]\n",
        "    last_hidden_sum = last_hidden.reshape(self.encoder.num_layers, 2, last_hidden.shape[1], -1).mean(dim=1)\n",
        "    if isinstance(x, PackedSequence):\n",
        "      hidden_mean = enc_hidden_state_by_t.data.reshape(-1, 2, last_hidden_sum.shape[-1]).mean(1)\n",
        "      enc_hidden_state_by_t = PackedSequence(hidden_mean, x[1], x[2], x[3])\n",
        "    else:\n",
        "      enc_hidden_state_by_t = enc_hidden_state_by_t.reshape(x.shape[0], x.shape[1], 2, -1).mean(dim=2)\n",
        "\n",
        "\n",
        "    return enc_hidden_state_by_t, last_hidden_sum\n",
        "\n",
        "  def run_decoder(self, y, last_hidden_state):\n",
        "    if isinstance(y, PackedSequence):\n",
        "      emb_y = PackedSequence(self.tgt_embedder(y.data), batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      emb_y = self.tgt_embedder(y)\n",
        "    out, decoder_last_hidden = self.decoder(emb_y, last_hidden_state)\n",
        "    return out, decoder_last_hidden\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "    y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    '''\n",
        "\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    out, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if isinstance(out, PackedSequence):\n",
        "      logits = self.decoder_proj(out.data)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      probs = PackedSequence(probs, batch_sizes=y.batch_sizes, sorted_indices=y.sorted_indices, unsorted_indices=y.unsorted_indices)\n",
        "    else:\n",
        "      logits = self.decoder_proj(out)\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "wG23j7DZ6UT5"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatorAtt(TranslatorBi):\n",
        "  def __init__(self, src_tokenizer, tgt_tokenizer, hidden_size=512, num_layers=3):\n",
        "    super().__init__(src_tokenizer, tgt_tokenizer, hidden_size, num_layers)\n",
        "\n",
        "    # define new self.decoder_proj\n",
        "    self.decoder_proj = nn.Linear(hidden_size * 2, self.tgt_vocab_size)\n",
        "\n",
        "  def get_attention_vector(self, encoder_hidden_states, decoder_hidden_states, mask):\n",
        "    '''\n",
        "    Arguments:\n",
        "      encoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of encoder GRU. Shape: [N, Ts, C]\n",
        "      decoder_hidden_states (torch.Tensor or PackedSequence): Hidden states of decoder GRU. Shape: [N, Tt, C]\n",
        "      mask (torch.Tensor): Masking tensor. If the mask value is 0, the attention weight has to be zero. Shape: [N, Ts]\n",
        "\n",
        "    Outputs:\n",
        "      attention_vectors (torch.Tensor or PackedSequence): Attention vectors that has the same shape as decoder_hidden_states\n",
        "      attention_weights (torch.Tensor): Zero-padded attention weights.\n",
        "                                You don't need to return it during the training, but it will help you to implement later problem\n",
        "\n",
        "    TODO: Complete this function using following functions\n",
        "      get_attention_score_for_a_batch_multiple_query\n",
        "      get_masked_softmax\n",
        "      get_batch_weighted_sum\n",
        "    If the inputs are PackedSequence, the output has to be a PackedSequence\n",
        "    Use torch.nn.utils.rnn.pad_packed_sequence(packed_sequence, batch_first=True) to convert PackedSequence to Tensor\n",
        "    Use torch.nn.utils.rnn.pack_padded_sequence(tensor, batch_lens, batch_first=True) to convert Tensor to PackedSequence\n",
        "    '''\n",
        "    is_packed = isinstance(encoder_hidden_states, PackedSequence)\n",
        "    if is_packed:\n",
        "      encoder_hidden_states, source_lens = pad_packed_sequence(encoder_hidden_states, batch_first=True)\n",
        "      decoder_hidden_states, target_lens = pad_packed_sequence(decoder_hidden_states, batch_first=True)\n",
        "\n",
        "    # Write your code from here\n",
        "\n",
        "    # 1. Calculate attention score using encoder_hidden_states and decoder_hidden_states\n",
        "    # 2. Mask the attention score using mask and apply softmax to get attention weight\n",
        "    # 3. Calculate attention vector using attention weight and encoder_hidden_states\n",
        "\n",
        "\n",
        "    #\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    '''\n",
        "    Arguments:\n",
        "      x (torch.Tensor or PackedSequence): Batch of source sentences\n",
        "      y (torch.Tensor or PackedSequence): Batch of target sentences\n",
        "    Output:\n",
        "      prob_dist (torch.Tensor or PackedSequence): Batch of probability distribution of word for target sentence\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "\n",
        "    is_packed = isinstance(x, PackedSequence)\n",
        "    enc_hidden_state_by_t, last_hidden_sum = self.run_encoder(x)\n",
        "    dec_hidden_state_by_t, decoder_last_hidden = self.run_decoder(y, last_hidden_sum)\n",
        "\n",
        "    if is_packed:\n",
        "      mask = pad_packed_sequence(x, batch_first=True)[0] != 0\n",
        "    else:\n",
        "      mask = torch.ones(x.shape[0], x.shape[1])\n",
        "\n",
        "    attention_vec, attention_weight = self.get_attention_vector(enc_hidden_state_by_t, dec_hidden_state_by_t, mask)\n",
        "\n",
        "    # TODO: Write your code from here\n",
        "    # CAUTION:\n",
        "    #   For the concatenation, you have to concat [dec_hidden_state_by_t; attention_vec], not [attention_vec; dec_hidden_state_by_t]\n",
        "    return\n",
        "\n",
        "\n",
        "model = TranslatorAtt(src_tokenizer, tgt_tokenizer, hidden_size=32, num_layers=2)\n",
        "\n",
        "model(batch[0], batch[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "rD8YJYF16UXm",
        "outputId": "61ebc23f-5748-44cb-9824-7a90f9cd9f1b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'batch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-441e05f66e8e>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslatorAtt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2c4xWPt66Ubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxEvDpb96UgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fg47PXKT6Uj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vEHW70bKylqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D9Qq1eLNlQ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0uI2vWRlQJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}